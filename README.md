# ğŸ•·ï¸ HaasCNC Web Scraper

An **asynchronous**, **resilient**, and **production-ready** web scraper targeting [https://www.haascnc.com](https://www.haascnc.com) â€“ English content only.

---

## ğŸš€ Project Goal

This scraper aims to:

- Aggressively crawl the HaasCNC website (ignoring robots.txt)
- Classify pages by **template type** (machine, promo, docs, etc.)
- Extract **main content** (excluding headers, footers, nav, cookie banners)
- Parse and extract **structured data**: tables, specs, PDFs, JSON-LD
- Store all extracted data in a **normalized SQLite database**

---

## ğŸ§± Project Structure

- `main.py` â€“ Example entry point for parsing a single page
- `crawl_machines.py` â€“ Main machine crawler (async, resumable, JSON export)
- `crawl_promos.py` â€“ Promo pages crawler
- `crawl_docs.py` â€“ Service/support docs crawler
- `parsers/` â€“ All page-type-specific parsers and template detection
- `utils/` â€“ Networking, proxies, user-agents, queue management
- `db/` â€“ DB schema and utility functions
- `tests/` â€“ Pytest suite for ignore-list, parsers, DB, proxies, queue
- `requirements.txt` â€“ Python dependencies
- `README.md` â€“ This file

---

## ğŸ”§ Installation

1. **Clone the repo**
2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```
3. (Optional) **Set up proxies** (see below)

---

## ğŸš¦ Usage

### Crawl all machines

```bash
python crawl_machines.py
```

### Crawl promos

```bash
python crawl_promos.py
```

### Crawl service/support docs

```bash
python crawl_docs.py
```

### Options

- Set `DRY_RUN = True` in the script to simulate without writing to DB
- Set `EXPORT_JSON = True` to export results as JSON

---

## ğŸŒ Proxy Setup

- Add your proxies (one per line) in a file named `proxies.txt` in the project root.
- Supported format: `http://user:pass@host:port` or `http://host:port`
- The scraper will rotate proxies and user-agents automatically.

---

## ğŸ—„ï¸ Database Structure

- **SQLite file**: `haasSiteData.db`
- **Tables**:
  - `machines` (id, url, title, subtitle, image_url, pdf_url, promo_text)
  - `machine_specs` (id, machine_id, label, value)
  - `promos` (id, url, title, promo_text)
  - `service_docs` (id, url, title, description, pdf_url)
- **Full-text indexes** for fast search

---

## ğŸ§ª Testing

- Run all tests:
  ```bash
  pytest tests/
  ```
- Coverage:
  - Ignore-list filtering
  - Parsers (machine, promo, service, doc)
  - Queue logic
  - DB utility functions
  - Proxy/user-agent rotation

---

## ğŸ“¦ Deliverables

- `haasSiteData.db` â€“ Example SQLite database
- `crawl_results.json`, `crawl_promos_results.json`, `crawl_docs_results.json` â€“ Example exports
- `crawler.log` â€“ Example log file
- `README.md`, `requirements.txt`
- `tests/` â€“ Full test suite

---

## ğŸ¤ Contact & Support

For questions, issues, or support, please contact the project maintainer.

---

## ğŸ› ï¸ Tech Stack

- Python 3.12+
- `httpx`, `asyncio` â€“ Async HTTP client
- `BeautifulSoup4` â€“ HTML parsing
- `sqlite3` â€“ Database
- `pytest` â€“ Testing
- `lxml`, `random`, `json`, etc.
